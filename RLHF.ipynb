{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1BNL6NVsahFtyRp0Z7aI7d5MESdn96QbC",
      "authorship_tag": "ABX9TyNG1Nxc1gT23N0lZUCPv/41",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atrbyg24/gpt2-rlhf/blob/main/RLHF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RLHF Fine-tuning**"
      ],
      "metadata": {
        "id": "8-4QBTp80Qss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the SFT and Reward models"
      ],
      "metadata": {
        "id": "NQjhpVdM0XEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/drive/MyDrive/reward_model.pt .\n",
        "%cp /content/drive/MyDrive/sft_model_epoch_1.zip ."
      ],
      "metadata": {
        "id": "r2rJjEYW-2hs"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -j sft_model_epoch_1.zip -d sft_model_epoch_1"
      ],
      "metadata": {
        "id": "h1Fwj-Am-ReD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward model**"
      ],
      "metadata": {
        "id": "mjBzzGiK0ake"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import Optional\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "class RewardHead(nn.Module):\n",
        "    \"\"\"\n",
        "    The RewardHead class implements a head for GPT2\n",
        "    that returns a scalar for each output token.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.reward = nn.Linear(self.hidden_size, 1)\n",
        "        self._post_init()\n",
        "\n",
        "    def _post_init(self):\n",
        "        nn.init.normal_(self.reward.weight, std=(1.0 / np.sqrt(self.hidden_size + 1)))\n",
        "        nn.init.zeros_(self.reward.bias)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        output = hidden_states\n",
        "        return self.reward(output)\n",
        "\n",
        "\n",
        "class GPT2RewardModel(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT2 model with a reward head on top.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.llm = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        # config = self.llm.config\n",
        "        # Add the reward head\n",
        "        self.reward_head = RewardHead(self.llm.config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "    ) -> Optional[torch.FloatTensor]:\n",
        "\n",
        "        transformer_outputs = self.llm.forward(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states = True,\n",
        "        )\n",
        "\n",
        "        # Get the last hidden state\n",
        "        last_hidden_state = transformer_outputs.hidden_states[-1]\n",
        "\n",
        "        # Apply the reward head\n",
        "        rewards = self.reward_head(last_hidden_state).squeeze(-1)\n",
        "\n",
        "        return torch.sigmoid(rewards)\n"
      ],
      "metadata": {
        "id": "Lfxuw1-xyeKK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "reward_model = GPT2RewardModel(model_name)\n",
        "reward_model.load_state_dict(torch.load(\"reward_model.pt\", map_location='cpu'))"
      ],
      "metadata": {
        "id": "D_CTF4-Pzbpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model with Value Head**"
      ],
      "metadata": {
        "id": "nFEA3WrP0f2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueHead(nn.Module):\n",
        "    \"\"\"\n",
        "    The ValueHead class implements a head for GPT2\n",
        "    that returns a scalar for each output token.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.value = nn.Linear(self.hidden_size, 1)\n",
        "        self._post_init()\n",
        "\n",
        "    def _post_init(self):\n",
        "        nn.init.normal_(self.value.weight, std=(1.0 / np.sqrt(self.hidden_size + 1)))\n",
        "        nn.init.zeros_(self.value.bias)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        output = hidden_states\n",
        "        return self.value(output)\n",
        "\n",
        "\n",
        "class ModelForCausalLMWithValueHead(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT2 model with a value head on top.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        super().__init__()\n",
        "        self.llm = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
        "        # config = self.llm.config\n",
        "        # Add the reward head\n",
        "        self.v_head = ValueHead(self.llm.config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "    ) -> Optional[torch.FloatTensor]:\n",
        "\n",
        "        transformer_outputs = self.llm.forward(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states = True,\n",
        "        )\n",
        "        lm_logits = transformer_outputs.logits\n",
        "        # Get the last hidden state\n",
        "        last_hidden_state = transformer_outputs.hidden_states[-1]\n",
        "\n",
        "        # Apply the reward head\n",
        "        value = self.v_head(last_hidden_state).squeeze(-1)\n",
        "        return lm_logits, value\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.llm.generate(*args, **kwargs)"
      ],
      "metadata": {
        "id": "f7LFdcCmzdM5"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hugging_face_token = userdata.get('hugging_face_read_token')\n",
        "model_path = './sft_model_epoch_1'\n",
        "model = ModelForCausalLMWithValueHead(model_path)"
      ],
      "metadata": {
        "id": "HRAGskrSzgoE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare dataset**"
      ],
      "metadata": {
        "id": "JQ-eABcv0jnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "0tf5ooF3ziPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets"
      ],
      "metadata": {
        "id": "QUfZV46Ozj4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"sst2\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "PtrVOcKE_3Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train, ds_val = dataset['train'], dataset['validation']"
      ],
      "metadata": {
        "id": "UkwOypsEzl1q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering**"
      ],
      "metadata": {
        "id": "jLu2xQ1B0pC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = ds_train.filter(lambda x: len(x['sentence'].split(' ')) > 8)"
      ],
      "metadata": {
        "id": "nQOeul5XzoxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val = ds_val.filter(lambda x: len(x['sentence'].split(' ')) > 8)"
      ],
      "metadata": {
        "id": "sMFSIniXzrTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "input_min_token_length = 2\n",
        "input_max_token_length = 8\n",
        "input_token_length_range = list(range(input_min_token_length, input_max_token_length))\n",
        "print(input_token_length_range)"
      ],
      "metadata": {
        "id": "_Cbu_zavzsm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.choice(input_token_length_range)"
      ],
      "metadata": {
        "id": "YSHLfqrczvkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sample):\n",
        "    input_size = random.choice(input_token_length_range)\n",
        "    sample['input_ids'] = tokenizer.encode(sample['sentence'])[:input_size]\n",
        "    sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
        "    sample['query'] = tokenizer.decode(sample['input_ids'])\n",
        "    return sample\n",
        "\n",
        "map_kwargs = {\n",
        "    \"batched\": False,\n",
        "    \"remove_columns\": ['idx', 'sentence', 'label']\n",
        "}\n",
        "\n",
        "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
        "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)"
      ],
      "metadata": {
        "id": "cWQYzLyizwCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_train.set_format(type='torch')\n",
        "tokenized_dataset_val.set_format(type='torch')"
      ],
      "metadata": {
        "id": "9idKwzA0z6xV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REWARD_TOKEN_ID = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "ooU_Aqbcz9_0"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def collator(batch):\n",
        "    return dict((key, [d[key] for d in batch]) for key in batch[0])\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset_train, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
        "val_dataloader = DataLoader(tokenized_dataset_val, batch_size=batch_size, collate_fn=collator, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Z8lWUwyn0AHx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "batch"
      ],
      "metadata": {
        "id": "Bq1QIDRP062X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_min_length = 5\n",
        "output_max_length = 16\n",
        "\n",
        "# https://huggingface.co/docs/trl/how_to_train#how-to-generate-text-for-training\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id\n",
        "}"
      ],
      "metadata": {
        "id": "Oo14woQJ0D8D"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Generation**"
      ],
      "metadata": {
        "id": "G1A7CcuZ0_fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "sample = tokenizer('Hi, this')\n",
        "sample"
      ],
      "metadata": {
        "id": "2ApCLpn90EgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_response = model.generate(\n",
        "    input_ids=torch.tensor(sample['input_ids']).unsqueeze(0),\n",
        "    attention_mask=torch.tensor(sample['attention_mask']).unsqueeze(0),\n",
        "    **generation_kwargs\n",
        "    ).squeeze(0)\n",
        "query_response"
      ],
      "metadata": {
        "id": "RO0tRsHf0GHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(query_response)"
      ],
      "metadata": {
        "id": "E6IQokow0I2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID])])\n",
        "    attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
        "    score = reward_model(query_response_score.unsqueeze(0), attention_mask.unsqueeze(0)).squeeze(0)[-1]\n",
        "score"
      ],
      "metadata": {
        "id": "Zo89YOy20KKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch generation**"
      ],
      "metadata": {
        "id": "kqCzuJNO1FV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "reward_model = reward_model.to(device)\n",
        "\n",
        "query_tensors = batch['input_ids']\n",
        "query_attention_masks = batch['attention_mask']\n",
        "\n",
        "response_tensors = []\n",
        "query_response_tensors = []\n",
        "score_tensors = []\n",
        "\n",
        "for i, query in enumerate(query_tensors):\n",
        "    query = query.to(device)\n",
        "    query_attention_mask = query_attention_masks[i].to(device)\n",
        "    new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "    generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "    query_response = model.generate(\n",
        "        input_ids=query.unsqueeze(0),\n",
        "        attention_mask=query_attention_mask.unsqueeze(0),\n",
        "        **generation_kwargs\n",
        "    ).squeeze(0)\n",
        "\n",
        "    response_len = len(query_response) - len(query)\n",
        "    response_tensors.append(query_response[-response_len:])\n",
        "    query_response_tensors.append(query_response)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
        "        attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
        "        score = reward_model(query_response_score.unsqueeze(0), attention_mask.unsqueeze(0)).squeeze(0)[-1]\n",
        "        score = 2 * (score - 0.5)\n",
        "    score_tensors.append(score)\n",
        "\n",
        "batch[\"response\"] = [tokenizer.decode(response) for response in response_tensors]\n",
        "print(batch['response'])"
      ],
      "metadata": {
        "id": "k2SOkouc0L3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Reward**"
      ],
      "metadata": {
        "id": "fJP7fvdR1IvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$reward =  score - \\log(\\frac{\\pi_{theta}^{RL}}{\\pi^{SFT}})$\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6ocHBbLU1LuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "sft_model = deepcopy(model)"
      ],
      "metadata": {
        "id": "0-2zP1yX1WGD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ebzr34X11dsc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = data_collator([\n",
        "    {'input_ids': ids,\n",
        "     'attention_mask': torch.ones_like(ids)} for ids in query_response_tensors\n",
        "]).to(device)\n",
        "input_data"
      ],
      "metadata": {
        "id": "iQ8RYPOo1fOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rewards(input_data, query_tensors, response_tensors, score_tensors):\n",
        "    with torch.no_grad():\n",
        "        logits, values = model(**input_data) # b, seq, vocab\n",
        "        ref_logits, _ = sft_model(**input_data)\n",
        "        logp = torch.nn.functional.log_softmax(logits[:, :-1, :], dim=-1)\n",
        "        ref_logp = torch.nn.functional.log_softmax(ref_logits[:, :-1, :], dim=-1)\n",
        "\n",
        "        labels = input_data['input_ids'][:, 1:] # b, seq\n",
        "\n",
        "        logp = torch.gather(logp, 2, labels.unsqueeze(-1)).squeeze(-1) # batch, seq\n",
        "        ref_logp = torch.gather(ref_logp, 2, labels.unsqueeze(-1)).squeeze(-1) # batch, seq\n",
        "\n",
        "        kl = logp - ref_logp\n",
        "        beta = 0.2\n",
        "        rewards = - beta * kl\n",
        "        attention_mask = input_data['attention_mask']\n",
        "        masks = torch.zeros_like(attention_mask[:, 1:])\n",
        "        masks[:,:] = attention_mask[:, 1:]\n",
        "        for j in range(len(query_tensors)):\n",
        "            start = len(query_tensors[j]) - 1\n",
        "            end = start + len(response_tensors[j])\n",
        "            masks[j, :start] = 0\n",
        "            masks[j, end:] = 0\n",
        "            rewards[j, end - 1] += score_tensors[j]\n",
        "            rewards[j, :] *= masks[j, :]\n",
        "            values[j, :-1] *= masks[j, :]\n",
        "\n",
        "    return logp, rewards, values[:, :-1], masks"
      ],
      "metadata": {
        "id": "5eDIJFBj1gWB"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs, rewards, values, masks = compute_rewards(input_data, query_tensors, response_tensors, score_tensors)\n",
        "print(rewards[0])\n",
        "print(input_data['input_ids'][0])\n",
        "print(input_data['attention_mask'][0])"
      ],
      "metadata": {
        "id": "Ig9Nq7HF1jjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(masks[0])\n",
        "print(values[0])"
      ],
      "metadata": {
        "id": "6X4jelI81k3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Advantage**"
      ],
      "metadata": {
        "id": "u4zae0yC1mjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_mean(values, mask):\n",
        "    return (values * mask).sum() / mask.sum()\n",
        "\n",
        "def masked_var(values, mask):\n",
        "    mean = masked_mean(values, mask)\n",
        "    centred_values = values - mean\n",
        "    return masked_mean(centred_values ** 2, mask)\n",
        "\n",
        "def masked_whiten(values, mask):\n",
        "    mean, var = masked_mean(values, mask), masked_var(values, mask)\n",
        "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
        "    whitened += mean\n",
        "    return whitened\n",
        "\n",
        "def compute_advantage(rewards, values, masks):\n",
        "    lastgae = 0.0\n",
        "    advantage_reversed = []\n",
        "    seq_length = rewards.shape[-1]\n",
        "    gamma, lam = 1.0, 0.95\n",
        "\n",
        "    for t in reversed(range(seq_length)):\n",
        "        nextvalues = values[:, t + 1] if t < seq_length - 1 else 0.0\n",
        "        delta = rewards[:, t] + gamma * nextvalues - values[:, t]\n",
        "        lastgae = delta + gamma * lam * lastgae\n",
        "        advantage_reversed.append(lastgae)\n",
        "    advantages = torch.stack(advantage_reversed[::-1], dim=1)\n",
        "    advantages = masked_whiten(advantages, masks)\n",
        "\n",
        "    returns = advantages + values\n",
        "    return advantages, returns"
      ],
      "metadata": {
        "id": "BwnNoq9N1mFA"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "advantages, returns = compute_advantage(rewards, values, masks)\n",
        "print(advantages[0])\n",
        "print(returns[0])"
      ],
      "metadata": {
        "id": "SJc8GFCe1q2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mini-batch PPO training**"
      ],
      "metadata": {
        "id": "8WYjQZs51sew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "WU7Spn9P1u6b"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.permutation(batch_size)"
      ],
      "metadata": {
        "id": "0mFUY9Vv1yrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batch_size = 4\n",
        "ppo_epochs = 4\n",
        "\n",
        "cliprange_ratio = 0.2\n",
        "\n",
        "v_loss_coeff = 0.1\n",
        "\n",
        "ratio_threshold = 10\n",
        "\n",
        "def compute_loss(old_logprobs, values, logprobs, vpreds, masks, advantages, returns):\n",
        "    ratio = torch.exp(logprobs - old_logprobs)\n",
        "    pg_loss1 = - ratio * advantages\n",
        "    pg_loss2 = - torch.clamp(ratio, 1 - cliprange_ratio, 1 + cliprange_ratio) * advantages\n",
        "    pg_loss = masked_mean(torch.max(pg_loss1, pg_loss2), masks)\n",
        "\n",
        "    v_loss = masked_mean((vpreds - returns) ** 2, masks)\n",
        "    loss = pg_loss + v_loss_coeff * v_loss\n",
        "\n",
        "    avg_ratio = masked_mean(ratio, masks)\n",
        "    if avg_ratio > ratio_threshold:\n",
        "        pg_loss = pg_loss * 0.0\n",
        "        v_loss = v_loss * 0.0\n",
        "        loss = loss * 0.0\n",
        "\n",
        "    return loss, v_loss\n",
        "\n",
        "def mini_batch_train():\n",
        "    for ep in range(ppo_epochs):\n",
        "        batch_inds = np.random.permutation(batch_size)\n",
        "\n",
        "        for start in range(0, batch_size, mini_batch_size):\n",
        "            end = start + mini_batch_size\n",
        "            mini_batch_inds = batch_inds[start:end]\n",
        "\n",
        "            mb_model_inputs = {\n",
        "                'input_ids': input_data['input_ids'][mini_batch_inds],\n",
        "                'attention_mask': input_data['attention_mask'][mini_batch_inds]\n",
        "            }\n",
        "            mb_logits, mb_vpreds = model(**mb_model_inputs)\n",
        "            mb_logits = torch.nn.functional.log_softmax(mb_logits[:, :-1, :], dim=-1)\n",
        "            mb_logprobs = torch.gather(mb_logits, 2, mb_model_inputs['input_ids'][:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            loss, loss_v = compute_loss(\n",
        "                logprobs[mini_batch_inds],\n",
        "                values[mini_batch_inds],\n",
        "                mb_logprobs,\n",
        "                mb_vpreds[:, :-1],\n",
        "                masks[mini_batch_inds],\n",
        "                advantages[mini_batch_inds],\n",
        "                returns[mini_batch_inds]\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print('loss/total', loss.item())\n",
        "    print('mini-batch training finished')\n"
      ],
      "metadata": {
        "id": "e7o1kh401z0x"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batch_train()"
      ],
      "metadata": {
        "id": "MnwXcV6Q12Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train RLHF**"
      ],
      "metadata": {
        "id": "v_ZV0Eq014BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        # Generate responses\n",
        "        query_tensors = batch['input_ids']\n",
        "        query_attention_masks = batch['attention_mask']\n",
        "\n",
        "        response_tensors = []\n",
        "        query_response_tensors = []\n",
        "        score_tensors = []\n",
        "\n",
        "        for i, query in enumerate(query_tensors):\n",
        "            query = query.to(device)\n",
        "            query_attention_mask = query_attention_masks[i].to(device)\n",
        "            new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
        "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "            query_response = model.generate(\n",
        "                input_ids=query.unsqueeze(0),\n",
        "                attention_mask=query_attention_mask.unsqueeze(0),\n",
        "                **generation_kwargs\n",
        "                ).squeeze(0)\n",
        "\n",
        "            response_len = len(query_response) - len(query)\n",
        "            response_tensors.append(query_response[-response_len:])\n",
        "            query_response_tensors.append(query_response)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
        "                attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
        "                score = reward_model(query_response_score.unsqueeze(0), attention_mask.unsqueeze(0)).squeeze(0)[-1]\n",
        "                score = 2 * (score - 0.5)\n",
        "            score_tensors.append(score)\n",
        "\n",
        "        input_data = data_collator([\n",
        "            {\n",
        "                'input_ids': ids,\n",
        "                'attention_mask': torch.ones_like(ids)\n",
        "            }\n",
        "            for ids in query_response_tensors\n",
        "        ]).to(device)\n",
        "\n",
        "        # rewards and advantages\n",
        "        logprobs, rewards, values, masks = compute_rewards(input_data, query_tensors, response_tensors, score_tensors)\n",
        "        advantages, returns = compute_advantage(rewards, values, masks)\n",
        "\n",
        "        # mini batch training\n",
        "        mini_batch_train()\n",
        "    print(f'epoch {epoch + 1} finished')\n"
      ],
      "metadata": {
        "id": "yf6bk7fc15bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation**"
      ],
      "metadata": {
        "id": "AojqV-AP2AYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_dataset_val)\n"
      ],
      "metadata": {
        "id": "-JVmjLVd2BwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_gen_lengths = [0] * len(tokenized_dataset_val)\n",
        "for i in range(len(tokenized_dataset_val)):\n",
        "    val_gen_lengths[i] = random.choice(list(range(output_min_length, output_max_length)))\n"
      ],
      "metadata": {
        "id": "kPUw6vnG2DDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate():\n",
        "    scores = []\n",
        "    for b, batch in enumerate(val_dataloader):\n",
        "        # Generate_responses\n",
        "        query_tensors = batch['input_ids']\n",
        "        query_attention_masks = batch['attention_mask']\n",
        "        for i, query in enumerate(query_tensors):\n",
        "            query = query.to(device)\n",
        "            query_attention_mask = query_attention_masks[i].to(device)\n",
        "            new_tokens = val_gen_lengths[b * len(query_tensors) + i]\n",
        "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
        "            query_response = model.generate(\n",
        "                input_ids=query.unsqueeze(0),\n",
        "                attention_mask=query_attention_mask.unsqueeze(0),\n",
        "                **generation_kwargs\n",
        "                ).squeeze(0)\n",
        "            query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
        "            attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
        "            score = reward_model(query_response_score.unsqueeze(0), attention_mask.unsqueeze(0)).squeeze(0)[-1]\n",
        "            score = 2 * (score - 0.5)\n",
        "            scores.append(score.item())\n",
        "    print('avg score:', sum(scores) / len(scores))\n"
      ],
      "metadata": {
        "id": "uPzluK1W2Fjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate()\n"
      ],
      "metadata": {
        "id": "NR9lTOi-2G8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'ppo_model_epoch_1.pt')\n"
      ],
      "metadata": {
        "id": "TWfOpYLt2IBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = './sft_model_epoch_1'\n",
        "model = ModelForCausalLMWithValueHead(model_path).to(device)"
      ],
      "metadata": {
        "id": "ieUfYFyu2JKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate()\n"
      ],
      "metadata": {
        "id": "v72AVoZi2Jwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OsFXqguz_-fF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}