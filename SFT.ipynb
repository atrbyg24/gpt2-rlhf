{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzFpUH0sqVh+5saJTw5ExD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atrbyg24/gpt2-rlhf/blob/main/SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Supervised Fine-Tuning**\n",
        "\n",
        "Supervised Fine-Tuning (SFT) is the first step in the entire RLHF fine-tuning pipeline (see Figure 2 in [RLHF paper](https://arxiv.org/pdf/2203.02155)). This notebook will use gpt2 and the corresponding tokenizer model from Hugging Face transformers library to perform SFT on stanfordnlp/sst2 dataset."
      ],
      "metadata": {
        "id": "91DAI3zGhVSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize gpt2 tokenizer and model**"
      ],
      "metadata": {
        "id": "RntF0wAiqquh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlHZsdgLhSqb"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hugging_face_token = userdata.get('hugging_face_read_token')\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_name = 'gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading a dataset**"
      ],
      "metadata": {
        "id": "ZdC2Jug6qv1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets"
      ],
      "metadata": {
        "id": "L9hhEUO5jotc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset_name = 'sst2'\n",
        "ds = load_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "ewCsklQwjtq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "6LsWFE6wkFL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train, ds_val = ds['train'], ds['validation']"
      ],
      "metadata": {
        "id": "eBiqCnZDkFjs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing a Dataset**"
      ],
      "metadata": {
        "id": "nqHyUQiBq0iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch['sentence'])\n",
        "\n",
        "map_kwargs = {\n",
        "    'batched':True,\n",
        "    'batch_size':512,\n",
        "    'remove_columns':['idx','sentence','label']\n",
        "}\n",
        "\n",
        "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
        "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)"
      ],
      "metadata": {
        "id": "IHO64X1rkxtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out sentences shorter than 5 tokens"
      ],
      "metadata": {
        "id": "B3k4NIMGq7M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_train = tokenized_dataset_train.filter(lambda x: len(x['input_ids']) > 5)\n",
        "tokenized_dataset_val = tokenized_dataset_val.filter(lambda x: len(x['input_ids']) > 5)"
      ],
      "metadata": {
        "id": "TAj6t6Wxl4QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_train.set_format('torch')\n",
        "tokenized_dataset_val.set_format('torch')"
      ],
      "metadata": {
        "id": "ut2qPok0mErX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "DA7xtvn5myDY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)\n",
        "\n",
        "dataloader_params = {\n",
        "    'batch_size':32,\n",
        "    'collate_fn':data_collator\n",
        "}\n",
        "\n",
        "dataloader_train = DataLoader(tokenized_dataset_train, **dataloader_params)\n",
        "dataloader_val = DataLoader(tokenized_dataset_val, **dataloader_params)"
      ],
      "metadata": {
        "id": "bab9xsfHmaD1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "model.to(device)\n",
        "num_epochs = 1"
      ],
      "metadata": {
        "id": "sIJ98i9pnXUq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        # iteration = epoch * len(dataloader_val) + i\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss # Uses transformers.loss.loss_utils.ForCausalLMLoss for loss calculation\n",
        "            total_loss += loss.item()\n",
        "    print(f'val_loss at {epoch} epoch:', total_loss / len(dataloader_val))"
      ],
      "metadata": {
        "id": "69marC_HpaXf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function code from [here](https://github.com/huggingface/transformers/blob/main/src/transformers/loss/loss_utils.py)"
      ],
      "metadata": {
        "id": "4FIBxLqNq-w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(dataloader_train):\n",
        "      batch = batch.to(device)\n",
        "      outputs = model(**batch)\n",
        "      loss = outputs.loss\n",
        "      print(f'Loss: {loss.item()}')\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    validate(epoch+1)"
      ],
      "metadata": {
        "id": "zhK5yWq_n3Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the model and zip saved model**"
      ],
      "metadata": {
        "id": "HKNF9-dAqjJA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "023cebf5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/sft_model_epoch_1')"
      ],
      "metadata": {
        "id": "5NjxlXBapt72"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.from_pretrained('/content/drive/MyDrive/sft_model_epoch_1')"
      ],
      "metadata": {
        "id": "mjdxU3-Opz7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/sft_model_epoch_1.zip /content/drive/MyDrive/sft_model_epoch_1"
      ],
      "metadata": {
        "id": "gGq8LDm_p11g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3hlWZ8w4fqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}